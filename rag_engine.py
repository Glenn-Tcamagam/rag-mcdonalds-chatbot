import os
import boto3
import json
import time

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableMap, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser


# ------------------------------------------------------------
# 0. R√âCUP√âRER LA CL√â OPENAI DEPUIS AWS SECRETS MANAGER
# ------------------------------------------------------------
def get_openai_key_from_aws():
    client = boto3.client("secretsmanager", region_name="eu-north-1")

    response = client.get_secret_value(SecretId="my_rag_secrets")
    secret = json.loads(response["SecretString"])

    return secret["OPENAI_API_KEY"]


# ------------------------------------------------------------
# 1. CHARGER UN PDF
# ------------------------------------------------------------
def load_pdf(path_pdf):
    loader = PyPDFLoader(path_pdf)
    return loader.load()


# ------------------------------------------------------------
# 2. SPLIT DES DOCUMENTS
# ------------------------------------------------------------
def split_documents(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=120
    )
    return splitter.split_documents(docs)


# ------------------------------------------------------------
# 3. VECTORSTORE + EMBEDDINGS OPENAI
# ------------------------------------------------------------
def create_vectorstore(splits):
    api_key = get_openai_key_from_aws()

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",
        openai_api_key=api_key
    )

    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory="chroma_db"
    )

    return vectorstore.as_retriever(search_kwargs={"k": 7})


# -------------------------------------------------------------------
# 4. M√âMOIRE DYNAMODB (PRO)
# -------------------------------------------------------------------
class DynamoDBMemory:
    def __init__(self, table_name="rag_memory", session_id="default"):
        self.session_id = session_id
        self.table = boto3.resource("dynamodb", region_name="eu-north-1").Table(table_name)

    def save_message(self, role, content):
        self.table.put_item(
            Item={
                "session_id": self.session_id,
                "timestamp": int(time.time() * 1000),
                "role": role,
                "content": content
            }
        )

    def load_messages(self):
        resp = self.table.query(
            KeyConditionExpression=boto3.dynamodb.conditions.Key("session_id").eq(self.session_id),
            ScanIndexForward=True
        )

        messages = resp.get("Items", [])

        history = ""
        for msg in messages:
            prefix = "Utilisateur: " if msg["role"] == "user" else "Assistant: "
            history += prefix + msg["content"] + "\n\n"

        return history.strip()


# ------------------------------------------------------------
# 5. RAG CHAIN AVEC M√âMOIRE
# ------------------------------------------------------------
def create_rag(retriever, session_id="default"):

    # M√©moire DynamoDB
    memory = DynamoDBMemory(session_id=session_id)

    api_key = get_openai_key_from_aws()
    os.environ["OPENAI_API_KEY"] = api_key

    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0
    )

    # Ajout de {memory} dans le prompt
    system_prompt = (
    "Tu es un assistant expert sp√©cialis√© EXCLUSIVEMENT dans les burgers de McDonald‚Äôs.\n"
    "Tu r√©ponds aux questions des utilisateurs en t‚Äôappuyant sur un syst√®me RAG professionnel.\n\n"

    "üìå SOURCES AUTORIS√âES (par ordre de priorit√©) :\n"
    "1Ô∏è‚É£ La m√©moire de la conversation (questions et r√©ponses pr√©c√©dentes)\n"
    "2Ô∏è‚É£ Les documents PDF McDonald‚Äôs index√©s (burgers, ingr√©dients, allerg√®nes, valeurs nutritionnelles, prix)\n"
    "3Ô∏è‚É£ Ton raisonnement logique UNIQUEMENT pour expliquer ou reformuler les informations trouv√©es\n\n"

    "üß† R√àGLES IMPORTANTES :\n"
    "- Tu r√©ponds UNIQUEMENT √† propos des burgers McDonald‚Äôs.\n"
    "- Si une information est pr√©sente dans les documents, tu dois la donner clairement et pr√©cis√©ment.\n"
    "- Tu ne dois JAMAIS inventer d‚Äôinformations.\n"
    "- Si l‚Äôinformation n‚Äôest pas pr√©sente dans les documents, dis-le explicitement.\n"
    "- Si la question concerne un autre produit que les burgers (boissons, desserts, menus enfants, etc.), pr√©cise que ce n‚Äôest pas couvert.\n\n"

    "üìÑ M√âMOIRE DE LA CONVERSATION :\n{memory}\n\n"

    "üìÑ EXTRAITS DES DOCUMENTS (RAG) :\n{context}\n\n"

    "‚úçÔ∏è STYLE DE R√âPONSE ATTENDU :\n"
    "- R√©ponse claire et structur√©e\n"
    "- Listes √† puces si n√©cessaire\n"
    "- Ton professionnel et p√©dagogique\n"
    "- R√©ponses compl√®tes mais concises\n"
)


    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])

    def format_docs(docs):
        return "\n\n".join([d.page_content for d in docs])

    # Cha√Æne RAG + m√©moire
    def rag_with_memory(user_input):
        conversation_memory = memory.load_messages()
        docs = retriever.invoke(user_input)
        context = format_docs(docs)

        inputs = {
            "memory": conversation_memory,
            "context": context,
            "input": user_input
        }

        answer = (prompt | llm | StrOutputParser()).invoke(inputs)

        # Sauvegarder en m√©moire
        memory.save_message("user", user_input)
        memory.save_message("assistant", answer)

        return answer

    return rag_with_memory



# -----------------------------
# Helper: construire la cha√Æne RAG compl√®te (utilis√©e par app.py)
# -----------------------------
import os  # si d√©j√† import√© dans le fichier, √ßa ne pose pas de probl√®me

def get_rag_chain(session_id: str = "client_1", pdf_folder: str = "pdfs"):
    """
    Convenience function used by app.py.
    - Charge automatiquement tous les PDFs du dossier `pdf_folder`
    - Split/embeddings/vectorstore
    - Cr√©e et retourne la fonction RAG (callable) pr√™te √† √™tre utilis√©e.
    """
    # 1) r√©cup√©rer fichiers pdf
    if not os.path.isdir(pdf_folder):
        raise ValueError(f"Dossier PDF introuvable: {pdf_folder}")

    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(".pdf")]
    if not pdf_files:
        raise ValueError(f"Aucun fichier PDF trouv√© dans {pdf_folder}.")

    # 2) charger tous les docs
    all_docs = []
    for pdf in pdf_files:
        path = os.path.join(pdf_folder, pdf)
        docs = load_pdf(path)
        all_docs.extend(docs)

    # 3) split + vectorstore
    splits = split_documents(all_docs)
    retriever = create_vectorstore(splits)

    # 4) cr√©er la cha√Æne RAG avec m√©moire (DynamoDB) ‚Äî create_rag doit accepter session_id
    #    create_rag retourne un callable (par ex. rag_with_memory)
    return create_rag(retriever, session_id=session_id)
